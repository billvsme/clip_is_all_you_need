{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e0601fe-c3cd-4d77-a451-284f3a1c87fc",
      "metadata": {
        "id": "9e0601fe-c3cd-4d77-a451-284f3a1c87fc"
      },
      "outputs": [],
      "source": [
        "\"\"\"安装依赖\n",
        "\"\"\"\n",
        "!pip install chromadb\n",
        "!pip install diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"下载模型\n",
        "\"\"\"\n",
        "!mkdir -p /content/modelsw\n",
        "!git clone --depth=1 https://huggingface.co/openai/clip-vit-large-patch14  /content/models/clip-vit-large-patch14\n",
        "!git clone --depth=1 https://huggingface.co/stabilityai/sdxl-turbo  /content/models/sdxl-turbo"
      ],
      "metadata": {
        "id": "1P3MCE69xBBH"
      },
      "id": "1P3MCE69xBBH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb6cd681-3d9e-4d29-bdcc-239184ad8104",
      "metadata": {
        "id": "fb6cd681-3d9e-4d29-bdcc-239184ad8104"
      },
      "outputs": [],
      "source": [
        "\"\"\"使用sdxl-turbo生成搜索用的图片\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "prompts = [\n",
        "    (\"cat\", \"a photo of cat\"),\n",
        "    (\"dog\", \"a photo of dog\"),\n",
        "    (\"pig\", \"a photo of pig\"),\n",
        "    (\"chair\", \"a photo of chair\"),\n",
        "    (\"table\", \"a photo of dining table\")\n",
        "]\n",
        "\n",
        "model_id = \"/content/models/sdxl-turbo\"\n",
        "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe.to(\"cuda\")\n",
        "\n",
        "for category, prompt in prompts:\n",
        "    os.makedirs(f\"output/{category}\", exist_ok=True)\n",
        "    for index in range(2):\n",
        "        image = pipe(prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
        "        image.save(f\"output/{category}/{category}-{index}.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e83b93d-e02c-48b0-84bd-7f2cd54c5c5c",
      "metadata": {
        "id": "2e83b93d-e02c-48b0-84bd-7f2cd54c5c5c"
      },
      "outputs": [],
      "source": [
        "\"\"\"初始化Chroma\n",
        "\"\"\"\n",
        "import chromadb\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"/content/chroma/my\")\n",
        "collection = client.get_or_create_collection(\n",
        "    name=\"my_collection\",\n",
        "    metadata={\"hnsw:space\": \"ip\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19038ded-0750-40ba-b2f9-b29261154e32",
      "metadata": {
        "id": "19038ded-0750-40ba-b2f9-b29261154e32"
      },
      "outputs": [],
      "source": [
        "\"\"\"图片向量存入Chroma\n",
        "\"\"\"\n",
        "import os\n",
        "from PIL import Image\n",
        "import requests\n",
        "import hashlib\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model_id = \"/content/models/clip-vit-large-patch14\"\n",
        "\n",
        "def all_images():\n",
        "    for root, ds, fs in os.walk(\"output\"):\n",
        "        for f in fs:\n",
        "            if f.endswith('.png'):\n",
        "                yield os.path.join(root, f)\n",
        "\n",
        "model = CLIPModel.from_pretrained(model_id)\n",
        "processor = CLIPProcessor.from_pretrained(model_id)\n",
        "\n",
        "for image_path in all_images():\n",
        "    image = Image.open(image_path)\n",
        "    inputs = processor(images=image.resize((224, 224)), return_tensors=\"pt\")\n",
        "    image_feature = model.get_image_features(**inputs)[0]\n",
        "\n",
        "    id_ = hashlib.md5(image.tobytes()).hexdigest()\n",
        "    collection.add(\n",
        "        embeddings=[image_feature.tolist()],\n",
        "        metadatas=[{\"source\": image_path, \"category\": image_path.split(\"/\")[1]}],\n",
        "        ids=[id_]\n",
        "    )\n",
        "\n",
        "    print(image_path, id_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f82837f4-75d5-4e46-9175-da61dec153be",
      "metadata": {
        "id": "f82837f4-75d5-4e46-9175-da61dec153be"
      },
      "outputs": [],
      "source": [
        "\"\"\"图片检索\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "# 文本 -> 图片检索\n",
        "queries = [\n",
        "    \"a photo of cat\",\n",
        "    \"a photo of dog\",\n",
        "    \"a photo of pig\",\n",
        "    \"a photo of chair\",\n",
        "    \"a photo of dining table\"\n",
        "]\n",
        "\n",
        "print(\"=\"*20)\n",
        "print(f\"文本 -> 图片检索\")\n",
        "for query in queries:\n",
        "    inputs = tokenizer([query], padding=True, return_tensors=\"pt\")\n",
        "    text_feature = model.get_text_features(**inputs)[0]\n",
        "    result = collection.query(\n",
        "        query_embeddings=[text_feature.tolist()],\n",
        "        n_results=2\n",
        "    )\n",
        "    print(f\"检索：{query}\")\n",
        "    for metadata in result[\"metadatas\"][0]:\n",
        "        print(\"image_path:\", metadata[\"source\"])\n",
        "\n",
        "# 图片->图片检索\n",
        "images = [\n",
        "    \"output/cat/cat-0.png\",\n",
        "    \"output/dog/dog-0.png\",\n",
        "    \"output/pig/pig-0.png\",\n",
        "    \"output/chair/chair-0.png\",\n",
        "    \"output/table/table-0.png\"\n",
        "]\n",
        "\n",
        "print(\"=\"*20)\n",
        "print(f\"图片 -> 图片检索\")\n",
        "for image_path in images:\n",
        "    image = Image.open(image_path)\n",
        "    inputs = processor(images=image.resize((224, 224)), return_tensors=\"pt\")\n",
        "    image_feature = model.get_image_features(**inputs)[0]\n",
        "\n",
        "    result = collection.query(\n",
        "        query_embeddings=[image_feature.tolist()],\n",
        "        n_results=2\n",
        "    )\n",
        "\n",
        "    print(f\"检索：{image_path}\")\n",
        "    for metadata in result[\"metadatas\"][0]:\n",
        "        print(\"image_path:\", metadata[\"source\"])\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}